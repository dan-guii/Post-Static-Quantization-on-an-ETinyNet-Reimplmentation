{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCv_m86YcYbT"
      },
      "outputs": [],
      "source": [
        "# PyTorch Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data transformation and loading\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((224, 224)), transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU7lkBjle1FP",
        "outputId": "56cb77ca-9911-4cd2-fbb2-2cf36dd59c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 42.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LBBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(LBBlock, self).__init__()\n",
        "        # First depthwise convolution (ϕ_d1) with batch normalization\n",
        "        self.depthwise1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        # Pointwise convolution (ϕ_p) with batch normalization\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Second depthwise convolution (ϕ_d2) with batch normalization\n",
        "        self.depthwise2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding, groups=out_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise1(x)                 # First depthwise convolution (ϕ_d1)\n",
        "        x = self.bn1(x)                        # Batch normalization after ϕ_d1\n",
        "        x = self.pointwise(x)                  # Pointwise convolution (ϕ_p)\n",
        "        x = self.bn2(x)                        # Batch normalization after ϕ_p\n",
        "        x = F.relu(x)                          # ReLU after pointwise\n",
        "        x = self.depthwise2(x)                 # Second depthwise convolution (ϕ_d2)\n",
        "        x = self.bn3(x)                        # Batch normalization after ϕ_d2\n",
        "        x = F.relu(x)                          # ReLU after second depthwise\n",
        "        return x\n",
        "\n",
        "class DLBBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(DLBBlock, self).__init__()\n",
        "        # 1x1 convolution to match dimensions if needed\n",
        "        if in_channels != out_channels:\n",
        "            self.match_channels = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        else:\n",
        "            self.match_channels = None  # Ensure it's always defined\n",
        "\n",
        "        # First depthwise convolution (ϕ_d1) with batch normalization only\n",
        "        self.depthwise1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        # Pointwise convolution (ϕ_p) with batch normalization and ReLU\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Second depthwise convolution (ϕ_d2) with batch normalization and ReLU\n",
        "        self.depthwise2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding, groups=out_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.match_channels(x) if self.match_channels else x   # Save input for the first shortcut connection\n",
        "\n",
        "        # First depthwise convolution (ϕ_d1)\n",
        "        x = self.depthwise1(x)\n",
        "        x = self.bn1(x)                              # Batch normalization only, no ReLU here\n",
        "\n",
        "        # Pointwise convolution (ϕ_p)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)                                # ReLU after pointwise\n",
        "\n",
        "        # First addition (adding the original input to the result after pointwise and ReLU)\n",
        "        x = x + residual                             # First shortcut connection\n",
        "        residual1 = x                                # Save the result for the second shortcut connection\n",
        "\n",
        "        # Second depthwise convolution (ϕ_d2)\n",
        "        x = self.depthwise2(x)\n",
        "        x = self.bn3(x)                              # Batch normalization after ϕ_d2\n",
        "        x = F.relu(x)                                # ReLU after second depthwise\n",
        "\n",
        "        # Final addition\n",
        "        x = x + residual\n",
        "        x = x + residual1                            # Final shortcut connection\n",
        "        return x"
      ],
      "metadata": {
        "id": "vT6L6mzIe3Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EtinyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EtinyNet, self).__init__()\n",
        "\n",
        "        # Initial 3x3 convolution with stride 2 to downsample\n",
        "        self.initial_conv = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # First pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # First set of LBBlocks: [32, 32, 32, 32] -> 56x56 feature map\n",
        "        self.lb1 = nn.Sequential(\n",
        "            LBBlock(32, 32),\n",
        "            LBBlock(32, 32),\n",
        "            LBBlock(32, 32),\n",
        "            LBBlock(32, 32)\n",
        "        )\n",
        "\n",
        "        # Second set of LBBlocks: [32, 128, 128, 128] -> 28x28 feature map\n",
        "        self.lb2 = nn.Sequential(\n",
        "            # First part: [32, 128, 128] x 1\n",
        "            LBBlock(32, 128),  # Expands channels from 32 to 128\n",
        "\n",
        "            # Second part: [128, 128, 128] x 3\n",
        "            LBBlock(128, 128),\n",
        "            LBBlock(128, 128),\n",
        "            LBBlock(128, 128)\n",
        "        )\n",
        "\n",
        "        # First DLBBlock: [128, 192, 192] -> 14x14 feature map\n",
        "        self.dlb1 = nn.Sequential(\n",
        "            DLBBlock(128, 192),\n",
        "            DLBBlock(192, 192),\n",
        "            DLBBlock(192, 192)\n",
        "        )\n",
        "\n",
        "        # Second DLBBlock: [192, 256, 256] -> 7x7 feature map\n",
        "        self.dlb2 = nn.Sequential(\n",
        "            DLBBlock(192, 256),\n",
        "            DLBBlock(256, 256),\n",
        "            DLBBlock(256, 512)\n",
        "        )\n",
        "\n",
        "        # Global average pooling (7x7 feature map to 1x1)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(512, 10)  # For CIFAR-10, which has 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape)\n",
        "        x = self.initial_conv(x)                  # Initial 3x3 convolution with stride 2, 112^2\n",
        "        #print(x.shape)\n",
        "        x = self.pool(x)                           # First pooling layer, 56^2\n",
        "        #print(x.shape)\n",
        "        x = self.lb1(x)                            # First set of LB blocks, 56^2\n",
        "        x = self.pool(x)                           # Pooling to reduce to 28x28\n",
        "        #print(x.shape)\n",
        "        x = self.lb2(x)                            # Second set of LB blocks\n",
        "        x = self.pool(x)                           # Pooling to reduce to 14x14\n",
        "        #print(x.shape)\n",
        "        x = self.dlb1(x)                           # First set of DLB blocks\n",
        "        x = self.pool(x)                           # Pooling to reduce to 7x7\n",
        "        #print(x.shape)\n",
        "        x = self.dlb2(x)                           # Second set of DLB blocks\n",
        "        x = self.global_avg_pool(x)                # Global average pooling to get 1x1 feature map\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1, 512)                        # Flatten for the fully connected layer\n",
        "        x = self.fc(x)                             # Fully connected layer\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "15WO3nrQe56w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Training on:\", device)\n",
        "\n",
        "# Instantiate the model, define loss function, optimizer, and learning rate scheduler\n",
        "net = EtinyNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)  # Reduce LR every 25 epochs\n",
        "\n",
        "num_epochs = 50  # Start with 50 epochs\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Training loop with validation\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    net.train()  # Set model to training mode\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Track training loss\n",
        "    train_losses.append(running_loss)\n",
        "\n",
        "    # Validation pass\n",
        "    net.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_losses.append(val_loss / len(testloader))\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    # Step learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v_IjckP9e7mf",
        "outputId": "08483965-81fc-43cc-8ced-48066ed7c69a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda\n",
            "Epoch: 1, Batch: 100, Loss: 12.7187\n",
            "Epoch: 1, Batch: 200, Loss: 1.9465\n",
            "Epoch: 1, Batch: 300, Loss: 1.7635\n",
            "Epoch: 1, Batch: 400, Loss: 1.7809\n",
            "Epoch: 1, Batch: 500, Loss: 1.7236\n",
            "Epoch: 1, Batch: 600, Loss: 1.6880\n",
            "Epoch: 1, Batch: 700, Loss: 1.6199\n",
            "Epoch: 1, Batch: 800, Loss: 1.5958\n",
            "Epoch: 1, Batch: 900, Loss: 1.6175\n",
            "Epoch: 1, Batch: 1000, Loss: 1.5144\n",
            "Epoch: 1, Batch: 1100, Loss: 1.5185\n",
            "Epoch: 1, Batch: 1200, Loss: 1.4534\n",
            "Epoch: 1, Batch: 1300, Loss: 1.4920\n",
            "Epoch: 1, Batch: 1400, Loss: 1.4470\n",
            "Epoch: 1, Batch: 1500, Loss: 1.4823\n",
            "Epoch 1, Validation Loss: 481.9044, Validation Accuracy: 46.09%\n",
            "Epoch: 2, Batch: 100, Loss: 1.4463\n",
            "Epoch: 2, Batch: 200, Loss: 1.3798\n",
            "Epoch: 2, Batch: 300, Loss: 1.3722\n",
            "Epoch: 2, Batch: 400, Loss: 1.3547\n",
            "Epoch: 2, Batch: 500, Loss: 1.3119\n",
            "Epoch: 2, Batch: 600, Loss: 1.3057\n",
            "Epoch: 2, Batch: 700, Loss: 1.3280\n",
            "Epoch: 2, Batch: 800, Loss: 1.3199\n",
            "Epoch: 2, Batch: 900, Loss: 1.3308\n",
            "Epoch: 2, Batch: 1000, Loss: 1.2183\n",
            "Epoch: 2, Batch: 1100, Loss: 1.2705\n",
            "Epoch: 2, Batch: 1200, Loss: 1.2132\n",
            "Epoch: 2, Batch: 1300, Loss: 1.2102\n",
            "Epoch: 2, Batch: 1400, Loss: 1.1335\n",
            "Epoch: 2, Batch: 1500, Loss: 1.1681\n",
            "Epoch 2, Validation Loss: 348.1944, Validation Accuracy: 59.83%\n",
            "Epoch: 3, Batch: 100, Loss: 1.1778\n",
            "Epoch: 3, Batch: 200, Loss: 1.1515\n",
            "Epoch: 3, Batch: 300, Loss: 1.1431\n",
            "Epoch: 3, Batch: 400, Loss: 1.1288\n",
            "Epoch: 3, Batch: 500, Loss: 1.1494\n",
            "Epoch: 3, Batch: 600, Loss: 1.1138\n",
            "Epoch: 3, Batch: 700, Loss: 1.0759\n",
            "Epoch: 3, Batch: 800, Loss: 1.0909\n",
            "Epoch: 3, Batch: 900, Loss: 1.0950\n",
            "Epoch: 3, Batch: 1000, Loss: 1.0906\n",
            "Epoch: 3, Batch: 1100, Loss: 1.0634\n",
            "Epoch: 3, Batch: 1200, Loss: 1.0503\n",
            "Epoch: 3, Batch: 1300, Loss: 1.0291\n",
            "Epoch: 3, Batch: 1400, Loss: 1.0093\n",
            "Epoch: 3, Batch: 1500, Loss: 1.0137\n",
            "Epoch 3, Validation Loss: 311.7585, Validation Accuracy: 63.99%\n",
            "Epoch: 4, Batch: 100, Loss: 0.9404\n",
            "Epoch: 4, Batch: 200, Loss: 0.9924\n",
            "Epoch: 4, Batch: 300, Loss: 0.9697\n",
            "Epoch: 4, Batch: 400, Loss: 0.9236\n",
            "Epoch: 4, Batch: 500, Loss: 0.9434\n",
            "Epoch: 4, Batch: 600, Loss: 0.9920\n",
            "Epoch: 4, Batch: 700, Loss: 0.9654\n",
            "Epoch: 4, Batch: 800, Loss: 0.9878\n",
            "Epoch: 4, Batch: 900, Loss: 0.9729\n",
            "Epoch: 4, Batch: 1000, Loss: 0.9455\n",
            "Epoch: 4, Batch: 1100, Loss: 0.9424\n",
            "Epoch: 4, Batch: 1200, Loss: 0.9255\n",
            "Epoch: 4, Batch: 1300, Loss: 0.8927\n",
            "Epoch: 4, Batch: 1400, Loss: 0.9087\n",
            "Epoch: 4, Batch: 1500, Loss: 0.8834\n",
            "Epoch 4, Validation Loss: 270.3092, Validation Accuracy: 70.45%\n",
            "Epoch: 5, Batch: 100, Loss: 0.9147\n",
            "Epoch: 5, Batch: 200, Loss: 0.8417\n",
            "Epoch: 5, Batch: 300, Loss: 0.8613\n",
            "Epoch: 5, Batch: 400, Loss: 0.8883\n",
            "Epoch: 5, Batch: 500, Loss: 0.8782\n",
            "Epoch: 5, Batch: 600, Loss: 0.8603\n",
            "Epoch: 5, Batch: 700, Loss: 0.8677\n",
            "Epoch: 5, Batch: 800, Loss: 0.8534\n",
            "Epoch: 5, Batch: 900, Loss: 0.8066\n",
            "Epoch: 5, Batch: 1000, Loss: 0.7974\n",
            "Epoch: 5, Batch: 1100, Loss: 0.8120\n",
            "Epoch: 5, Batch: 1200, Loss: 0.8134\n",
            "Epoch: 5, Batch: 1300, Loss: 0.8482\n",
            "Epoch: 5, Batch: 1400, Loss: 0.8477\n",
            "Epoch: 5, Batch: 1500, Loss: 0.8371\n",
            "Epoch 5, Validation Loss: 237.6923, Validation Accuracy: 73.89%\n",
            "Epoch: 6, Batch: 100, Loss: 0.7605\n",
            "Epoch: 6, Batch: 200, Loss: 0.7861\n",
            "Epoch: 6, Batch: 300, Loss: 0.7532\n",
            "Epoch: 6, Batch: 400, Loss: 0.7309\n",
            "Epoch: 6, Batch: 500, Loss: 0.7569\n",
            "Epoch: 6, Batch: 600, Loss: 0.7479\n",
            "Epoch: 6, Batch: 700, Loss: 0.7207\n",
            "Epoch: 6, Batch: 800, Loss: 0.7728\n",
            "Epoch: 6, Batch: 900, Loss: 0.7362\n",
            "Epoch: 6, Batch: 1000, Loss: 0.7622\n",
            "Epoch: 6, Batch: 1100, Loss: 0.7554\n",
            "Epoch: 6, Batch: 1200, Loss: 0.7301\n",
            "Epoch: 6, Batch: 1300, Loss: 0.7298\n",
            "Epoch: 6, Batch: 1400, Loss: 0.7084\n",
            "Epoch: 6, Batch: 1500, Loss: 0.7344\n",
            "Epoch 6, Validation Loss: 237.8028, Validation Accuracy: 74.13%\n",
            "Epoch: 7, Batch: 100, Loss: 0.6750\n",
            "Epoch: 7, Batch: 200, Loss: 0.6803\n",
            "Epoch: 7, Batch: 300, Loss: 0.6412\n",
            "Epoch: 7, Batch: 400, Loss: 0.6549\n",
            "Epoch: 7, Batch: 500, Loss: 0.6914\n",
            "Epoch: 7, Batch: 600, Loss: 0.6714\n",
            "Epoch: 7, Batch: 700, Loss: 0.6590\n",
            "Epoch: 7, Batch: 800, Loss: 0.6344\n",
            "Epoch: 7, Batch: 900, Loss: 0.6482\n",
            "Epoch: 7, Batch: 1000, Loss: 0.6441\n",
            "Epoch: 7, Batch: 1100, Loss: 0.7044\n",
            "Epoch: 7, Batch: 1200, Loss: 0.6655\n",
            "Epoch: 7, Batch: 1300, Loss: 0.6516\n",
            "Epoch: 7, Batch: 1400, Loss: 0.6612\n",
            "Epoch: 7, Batch: 1500, Loss: 0.6560\n",
            "Epoch 7, Validation Loss: 193.4490, Validation Accuracy: 78.64%\n",
            "Epoch: 8, Batch: 100, Loss: 0.5744\n",
            "Epoch: 8, Batch: 200, Loss: 0.6294\n",
            "Epoch: 8, Batch: 300, Loss: 0.6459\n",
            "Epoch: 8, Batch: 400, Loss: 0.5907\n",
            "Epoch: 8, Batch: 500, Loss: 0.6005\n",
            "Epoch: 8, Batch: 600, Loss: 0.5594\n",
            "Epoch: 8, Batch: 700, Loss: 0.5995\n",
            "Epoch: 8, Batch: 800, Loss: 0.5945\n",
            "Epoch: 8, Batch: 900, Loss: 0.6067\n",
            "Epoch: 8, Batch: 1000, Loss: 0.6034\n",
            "Epoch: 8, Batch: 1100, Loss: 0.5894\n",
            "Epoch: 8, Batch: 1200, Loss: 0.6175\n",
            "Epoch: 8, Batch: 1300, Loss: 0.6069\n",
            "Epoch: 8, Batch: 1400, Loss: 0.6448\n",
            "Epoch: 8, Batch: 1500, Loss: 0.5765\n",
            "Epoch 8, Validation Loss: 182.8903, Validation Accuracy: 79.71%\n",
            "Epoch: 9, Batch: 100, Loss: 0.5168\n",
            "Epoch: 9, Batch: 200, Loss: 0.5312\n",
            "Epoch: 9, Batch: 300, Loss: 0.5371\n",
            "Epoch: 9, Batch: 400, Loss: 0.5303\n",
            "Epoch: 9, Batch: 500, Loss: 0.5272\n",
            "Epoch: 9, Batch: 600, Loss: 0.5500\n",
            "Epoch: 9, Batch: 700, Loss: 0.5570\n",
            "Epoch: 9, Batch: 800, Loss: 0.5625\n",
            "Epoch: 9, Batch: 900, Loss: 0.5862\n",
            "Epoch: 9, Batch: 1000, Loss: 0.5481\n",
            "Epoch: 9, Batch: 1100, Loss: 0.5274\n",
            "Epoch: 9, Batch: 1200, Loss: 0.5087\n",
            "Epoch: 9, Batch: 1300, Loss: 0.5414\n",
            "Epoch: 9, Batch: 1400, Loss: 0.5808\n",
            "Epoch: 9, Batch: 1500, Loss: 0.5753\n",
            "Epoch 9, Validation Loss: 196.1221, Validation Accuracy: 78.81%\n",
            "Epoch: 10, Batch: 100, Loss: 0.5117\n",
            "Epoch: 10, Batch: 200, Loss: 0.4360\n",
            "Epoch: 10, Batch: 300, Loss: 0.5296\n",
            "Epoch: 10, Batch: 400, Loss: 0.5263\n",
            "Epoch: 10, Batch: 500, Loss: 0.4957\n",
            "Epoch: 10, Batch: 600, Loss: 0.4880\n",
            "Epoch: 10, Batch: 700, Loss: 0.4953\n",
            "Epoch: 10, Batch: 800, Loss: 0.4951\n",
            "Epoch: 10, Batch: 900, Loss: 0.5242\n",
            "Epoch: 10, Batch: 1000, Loss: 0.5511\n",
            "Epoch: 10, Batch: 1100, Loss: 0.4933\n",
            "Epoch: 10, Batch: 1200, Loss: 0.5122\n",
            "Epoch: 10, Batch: 1300, Loss: 0.5356\n",
            "Epoch: 10, Batch: 1400, Loss: 0.5037\n",
            "Epoch: 10, Batch: 1500, Loss: 0.4806\n",
            "Epoch 10, Validation Loss: 173.1277, Validation Accuracy: 81.39%\n",
            "Epoch: 11, Batch: 100, Loss: 0.4090\n",
            "Epoch: 11, Batch: 200, Loss: 0.4603\n",
            "Epoch: 11, Batch: 300, Loss: 0.4222\n",
            "Epoch: 11, Batch: 400, Loss: 0.4850\n",
            "Epoch: 11, Batch: 500, Loss: 0.4577\n",
            "Epoch: 11, Batch: 600, Loss: 0.4439\n",
            "Epoch: 11, Batch: 700, Loss: 0.4733\n",
            "Epoch: 11, Batch: 800, Loss: 0.4897\n",
            "Epoch: 11, Batch: 900, Loss: 0.4529\n",
            "Epoch: 11, Batch: 1000, Loss: 0.4561\n",
            "Epoch: 11, Batch: 1100, Loss: 0.4632\n",
            "Epoch: 11, Batch: 1200, Loss: 0.4770\n",
            "Epoch: 11, Batch: 1300, Loss: 0.5004\n",
            "Epoch: 11, Batch: 1400, Loss: 0.4445\n",
            "Epoch: 11, Batch: 1500, Loss: 0.4679\n",
            "Epoch 11, Validation Loss: 168.1262, Validation Accuracy: 82.41%\n",
            "Epoch: 12, Batch: 100, Loss: 0.3691\n",
            "Epoch: 12, Batch: 200, Loss: 0.3835\n",
            "Epoch: 12, Batch: 300, Loss: 0.4450\n",
            "Epoch: 12, Batch: 400, Loss: 0.3923\n",
            "Epoch: 12, Batch: 500, Loss: 0.4115\n",
            "Epoch: 12, Batch: 600, Loss: 0.3984\n",
            "Epoch: 12, Batch: 700, Loss: 0.4089\n",
            "Epoch: 12, Batch: 800, Loss: 0.4619\n",
            "Epoch: 12, Batch: 900, Loss: 0.4341\n",
            "Epoch: 12, Batch: 1000, Loss: 0.4468\n",
            "Epoch: 12, Batch: 1100, Loss: 0.4426\n",
            "Epoch: 12, Batch: 1200, Loss: 0.4430\n",
            "Epoch: 12, Batch: 1300, Loss: 0.4376\n",
            "Epoch: 12, Batch: 1400, Loss: 0.4879\n",
            "Epoch: 12, Batch: 1500, Loss: 0.4718\n",
            "Epoch 12, Validation Loss: 173.8725, Validation Accuracy: 81.80%\n",
            "Epoch: 13, Batch: 100, Loss: 0.3398\n",
            "Epoch: 13, Batch: 200, Loss: 0.3460\n",
            "Epoch: 13, Batch: 300, Loss: 0.3396\n",
            "Epoch: 13, Batch: 400, Loss: 0.4029\n",
            "Epoch: 13, Batch: 500, Loss: 0.4141\n",
            "Epoch: 13, Batch: 600, Loss: 0.4296\n",
            "Epoch: 13, Batch: 700, Loss: 0.3826\n",
            "Epoch: 13, Batch: 800, Loss: 0.3432\n",
            "Epoch: 13, Batch: 900, Loss: 0.4003\n",
            "Epoch: 13, Batch: 1000, Loss: 0.3951\n",
            "Epoch: 13, Batch: 1100, Loss: 0.4432\n",
            "Epoch: 13, Batch: 1200, Loss: 0.4134\n",
            "Epoch: 13, Batch: 1300, Loss: 0.4007\n",
            "Epoch: 13, Batch: 1400, Loss: 0.4062\n",
            "Epoch: 13, Batch: 1500, Loss: 0.4169\n",
            "Epoch 13, Validation Loss: 178.1401, Validation Accuracy: 81.38%\n",
            "Epoch: 14, Batch: 100, Loss: 0.2929\n",
            "Epoch: 14, Batch: 200, Loss: 0.3046\n",
            "Epoch: 14, Batch: 300, Loss: 0.3367\n",
            "Epoch: 14, Batch: 400, Loss: 0.3737\n",
            "Epoch: 14, Batch: 500, Loss: 0.3740\n",
            "Epoch: 14, Batch: 600, Loss: 0.3920\n",
            "Epoch: 14, Batch: 700, Loss: 0.3635\n",
            "Epoch: 14, Batch: 800, Loss: 0.4162\n",
            "Epoch: 14, Batch: 900, Loss: 0.3822\n",
            "Epoch: 14, Batch: 1000, Loss: 0.3774\n",
            "Epoch: 14, Batch: 1100, Loss: 0.3760\n",
            "Epoch: 14, Batch: 1200, Loss: 0.3262\n",
            "Epoch: 14, Batch: 1300, Loss: 0.3901\n",
            "Epoch: 14, Batch: 1400, Loss: 0.4107\n",
            "Epoch: 14, Batch: 1500, Loss: 0.3946\n",
            "Epoch 14, Validation Loss: 168.7919, Validation Accuracy: 82.66%\n",
            "Epoch: 15, Batch: 100, Loss: 0.2761\n",
            "Epoch: 15, Batch: 200, Loss: 0.2806\n",
            "Epoch: 15, Batch: 300, Loss: 0.3062\n",
            "Epoch: 15, Batch: 400, Loss: 0.3146\n",
            "Epoch: 15, Batch: 500, Loss: 0.3439\n",
            "Epoch: 15, Batch: 600, Loss: 0.3494\n",
            "Epoch: 15, Batch: 700, Loss: 0.3491\n",
            "Epoch: 15, Batch: 800, Loss: 0.3549\n",
            "Epoch: 15, Batch: 900, Loss: 0.3412\n",
            "Epoch: 15, Batch: 1000, Loss: 0.3823\n",
            "Epoch: 15, Batch: 1100, Loss: 0.3421\n",
            "Epoch: 15, Batch: 1200, Loss: 0.3426\n",
            "Epoch: 15, Batch: 1300, Loss: 0.3279\n",
            "Epoch: 15, Batch: 1400, Loss: 0.3546\n",
            "Epoch: 15, Batch: 1500, Loss: 0.3707\n",
            "Epoch 15, Validation Loss: 179.4404, Validation Accuracy: 82.17%\n",
            "Epoch: 16, Batch: 100, Loss: 0.2632\n",
            "Epoch: 16, Batch: 200, Loss: 0.2757\n",
            "Epoch: 16, Batch: 300, Loss: 0.2654\n",
            "Epoch: 16, Batch: 400, Loss: 0.3062\n",
            "Epoch: 16, Batch: 500, Loss: 0.3035\n",
            "Epoch: 16, Batch: 600, Loss: 0.3199\n",
            "Epoch: 16, Batch: 700, Loss: 0.3559\n",
            "Epoch: 16, Batch: 800, Loss: 0.3484\n",
            "Epoch: 16, Batch: 900, Loss: 0.3640\n",
            "Epoch: 16, Batch: 1000, Loss: 0.3382\n",
            "Epoch: 16, Batch: 1100, Loss: 0.3593\n",
            "Epoch: 16, Batch: 1200, Loss: 0.3139\n",
            "Epoch: 16, Batch: 1300, Loss: 0.2977\n",
            "Epoch: 16, Batch: 1400, Loss: 0.3233\n",
            "Epoch: 16, Batch: 1500, Loss: 0.3188\n",
            "Epoch 16, Validation Loss: 170.4934, Validation Accuracy: 82.97%\n",
            "Epoch: 17, Batch: 100, Loss: 0.2657\n",
            "Epoch: 17, Batch: 200, Loss: 0.2476\n",
            "Epoch: 17, Batch: 300, Loss: 0.2386\n",
            "Epoch: 17, Batch: 400, Loss: 0.2606\n",
            "Epoch: 17, Batch: 500, Loss: 0.3220\n",
            "Epoch: 17, Batch: 600, Loss: 0.2747\n",
            "Epoch: 17, Batch: 700, Loss: 0.3438\n",
            "Epoch: 17, Batch: 800, Loss: 0.3320\n",
            "Epoch: 17, Batch: 900, Loss: 0.3234\n",
            "Epoch: 17, Batch: 1000, Loss: 0.3028\n",
            "Epoch: 17, Batch: 1100, Loss: 0.2922\n",
            "Epoch: 17, Batch: 1200, Loss: 0.3405\n",
            "Epoch: 17, Batch: 1300, Loss: 0.3080\n",
            "Epoch: 17, Batch: 1400, Loss: 0.2945\n",
            "Epoch: 17, Batch: 1500, Loss: 0.3425\n",
            "Epoch 17, Validation Loss: 164.9777, Validation Accuracy: 83.37%\n",
            "Epoch: 18, Batch: 100, Loss: 0.2147\n",
            "Epoch: 18, Batch: 200, Loss: 0.2304\n",
            "Epoch: 18, Batch: 300, Loss: 0.2389\n",
            "Epoch: 18, Batch: 400, Loss: 0.2097\n",
            "Epoch: 18, Batch: 500, Loss: 0.2799\n",
            "Epoch: 18, Batch: 600, Loss: 0.2763\n",
            "Epoch: 18, Batch: 700, Loss: 0.2775\n",
            "Epoch: 18, Batch: 800, Loss: 0.2811\n",
            "Epoch: 18, Batch: 900, Loss: 0.2764\n",
            "Epoch: 18, Batch: 1000, Loss: 0.3081\n",
            "Epoch: 18, Batch: 1100, Loss: 0.2863\n",
            "Epoch: 18, Batch: 1200, Loss: 0.3092\n",
            "Epoch: 18, Batch: 1300, Loss: 0.2897\n",
            "Epoch: 18, Batch: 1400, Loss: 0.2881\n",
            "Epoch: 18, Batch: 1500, Loss: 0.2848\n",
            "Epoch 18, Validation Loss: 158.4239, Validation Accuracy: 84.40%\n",
            "Epoch: 19, Batch: 100, Loss: 0.1979\n",
            "Epoch: 19, Batch: 200, Loss: 0.1956\n",
            "Epoch: 19, Batch: 300, Loss: 0.2166\n",
            "Epoch: 19, Batch: 400, Loss: 0.2405\n",
            "Epoch: 19, Batch: 500, Loss: 0.2485\n",
            "Epoch: 19, Batch: 600, Loss: 0.2857\n",
            "Epoch: 19, Batch: 700, Loss: 0.2621\n",
            "Epoch: 19, Batch: 800, Loss: 0.2714\n",
            "Epoch: 19, Batch: 900, Loss: 0.2603\n",
            "Epoch: 19, Batch: 1000, Loss: 0.2740\n",
            "Epoch: 19, Batch: 1100, Loss: 0.2887\n",
            "Epoch: 19, Batch: 1200, Loss: 0.2534\n",
            "Epoch: 19, Batch: 1300, Loss: 0.2789\n",
            "Epoch: 19, Batch: 1400, Loss: 0.2929\n",
            "Epoch: 19, Batch: 1500, Loss: 0.2624\n",
            "Epoch 19, Validation Loss: 175.7139, Validation Accuracy: 83.27%\n",
            "Epoch: 20, Batch: 100, Loss: 0.1791\n",
            "Epoch: 20, Batch: 200, Loss: 0.1959\n",
            "Epoch: 20, Batch: 300, Loss: 0.1930\n",
            "Epoch: 20, Batch: 400, Loss: 0.2215\n",
            "Epoch: 20, Batch: 500, Loss: 0.2326\n",
            "Epoch: 20, Batch: 600, Loss: 0.2430\n",
            "Epoch: 20, Batch: 700, Loss: 0.2679\n",
            "Epoch: 20, Batch: 800, Loss: 0.2695\n",
            "Epoch: 20, Batch: 900, Loss: 0.2582\n",
            "Epoch: 20, Batch: 1000, Loss: 0.2335\n",
            "Epoch: 20, Batch: 1100, Loss: 0.2263\n",
            "Epoch: 20, Batch: 1200, Loss: 0.2625\n",
            "Epoch: 20, Batch: 1300, Loss: 0.2786\n",
            "Epoch: 20, Batch: 1400, Loss: 0.2834\n",
            "Epoch: 20, Batch: 1500, Loss: 0.2916\n",
            "Epoch 20, Validation Loss: 186.2720, Validation Accuracy: 82.96%\n",
            "Epoch: 21, Batch: 100, Loss: 0.2116\n",
            "Epoch: 21, Batch: 200, Loss: 0.1758\n",
            "Epoch: 21, Batch: 300, Loss: 0.1883\n",
            "Epoch: 21, Batch: 400, Loss: 0.1947\n",
            "Epoch: 21, Batch: 500, Loss: 0.1956\n",
            "Epoch: 21, Batch: 600, Loss: 0.2016\n",
            "Epoch: 21, Batch: 700, Loss: 0.2332\n",
            "Epoch: 21, Batch: 800, Loss: 0.2452\n",
            "Epoch: 21, Batch: 900, Loss: 0.2132\n",
            "Epoch: 21, Batch: 1000, Loss: 0.2202\n",
            "Epoch: 21, Batch: 1100, Loss: 0.2010\n",
            "Epoch: 21, Batch: 1200, Loss: 0.2384\n",
            "Epoch: 21, Batch: 1300, Loss: 0.2611\n",
            "Epoch: 21, Batch: 1400, Loss: 0.2689\n",
            "Epoch: 21, Batch: 1500, Loss: 0.2527\n",
            "Epoch 21, Validation Loss: 183.7571, Validation Accuracy: 83.91%\n",
            "Epoch: 22, Batch: 100, Loss: 0.1806\n",
            "Epoch: 22, Batch: 200, Loss: 0.1914\n",
            "Epoch: 22, Batch: 300, Loss: 0.1692\n",
            "Epoch: 22, Batch: 400, Loss: 0.1925\n",
            "Epoch: 22, Batch: 500, Loss: 0.1909\n",
            "Epoch: 22, Batch: 600, Loss: 0.2401\n",
            "Epoch: 22, Batch: 700, Loss: 0.2104\n",
            "Epoch: 22, Batch: 800, Loss: 0.2281\n",
            "Epoch: 22, Batch: 900, Loss: 0.2213\n",
            "Epoch: 22, Batch: 1000, Loss: 0.2359\n",
            "Epoch: 22, Batch: 1100, Loss: 0.2362\n",
            "Epoch: 22, Batch: 1200, Loss: 0.2331\n",
            "Epoch: 22, Batch: 1300, Loss: 0.2438\n",
            "Epoch: 22, Batch: 1400, Loss: 0.2594\n",
            "Epoch: 22, Batch: 1500, Loss: 0.2683\n",
            "Epoch 22, Validation Loss: 175.3192, Validation Accuracy: 84.26%\n",
            "Epoch: 23, Batch: 100, Loss: 0.1414\n",
            "Epoch: 23, Batch: 200, Loss: 0.1605\n",
            "Epoch: 23, Batch: 300, Loss: 0.1729\n",
            "Epoch: 23, Batch: 400, Loss: 0.1608\n",
            "Epoch: 23, Batch: 500, Loss: 0.1724\n",
            "Epoch: 23, Batch: 600, Loss: 0.2212\n",
            "Epoch: 23, Batch: 700, Loss: 0.2099\n",
            "Epoch: 23, Batch: 800, Loss: 0.2041\n",
            "Epoch: 23, Batch: 900, Loss: 0.2391\n",
            "Epoch: 23, Batch: 1000, Loss: 0.2253\n",
            "Epoch: 23, Batch: 1100, Loss: 0.2433\n",
            "Epoch: 23, Batch: 1200, Loss: 0.2309\n",
            "Epoch: 23, Batch: 1300, Loss: 0.2217\n",
            "Epoch: 23, Batch: 1400, Loss: 0.2361\n",
            "Epoch: 23, Batch: 1500, Loss: 0.2266\n",
            "Epoch 23, Validation Loss: 190.9360, Validation Accuracy: 83.24%\n",
            "Epoch: 24, Batch: 100, Loss: 0.1728\n",
            "Epoch: 24, Batch: 200, Loss: 0.1660\n",
            "Epoch: 24, Batch: 300, Loss: 0.1733\n",
            "Epoch: 24, Batch: 400, Loss: 0.1758\n",
            "Epoch: 24, Batch: 500, Loss: 0.1909\n",
            "Epoch: 24, Batch: 600, Loss: 0.1934\n",
            "Epoch: 24, Batch: 700, Loss: 0.1717\n",
            "Epoch: 24, Batch: 800, Loss: 0.2192\n",
            "Epoch: 24, Batch: 900, Loss: 0.2202\n",
            "Epoch: 24, Batch: 1000, Loss: 0.2069\n",
            "Epoch: 24, Batch: 1100, Loss: 0.2382\n",
            "Epoch: 24, Batch: 1200, Loss: 0.2125\n",
            "Epoch: 24, Batch: 1300, Loss: 0.2187\n",
            "Epoch: 24, Batch: 1400, Loss: 0.2169\n",
            "Epoch: 24, Batch: 1500, Loss: 0.2246\n",
            "Epoch 24, Validation Loss: 182.4905, Validation Accuracy: 84.32%\n",
            "Epoch: 25, Batch: 100, Loss: 0.1501\n",
            "Epoch: 25, Batch: 200, Loss: 0.1570\n",
            "Epoch: 25, Batch: 300, Loss: 0.1549\n",
            "Epoch: 25, Batch: 400, Loss: 0.1399\n",
            "Epoch: 25, Batch: 500, Loss: 0.1731\n",
            "Epoch: 25, Batch: 600, Loss: 0.1816\n",
            "Epoch: 25, Batch: 700, Loss: 0.1999\n",
            "Epoch: 25, Batch: 800, Loss: 0.1834\n",
            "Epoch: 25, Batch: 900, Loss: 0.2123\n",
            "Epoch: 25, Batch: 1000, Loss: 0.2094\n",
            "Epoch: 25, Batch: 1100, Loss: 0.2238\n",
            "Epoch: 25, Batch: 1200, Loss: 0.2193\n",
            "Epoch: 25, Batch: 1300, Loss: 0.1974\n",
            "Epoch: 25, Batch: 1400, Loss: 0.2112\n",
            "Epoch: 25, Batch: 1500, Loss: 0.2278\n",
            "Epoch 25, Validation Loss: 177.0716, Validation Accuracy: 84.32%\n",
            "Epoch: 26, Batch: 100, Loss: 0.1170\n",
            "Epoch: 26, Batch: 200, Loss: 0.0956\n",
            "Epoch: 26, Batch: 300, Loss: 0.0879\n",
            "Epoch: 26, Batch: 400, Loss: 0.0774\n",
            "Epoch: 26, Batch: 500, Loss: 0.0779\n",
            "Epoch: 26, Batch: 600, Loss: 0.0792\n",
            "Epoch: 26, Batch: 700, Loss: 0.0837\n",
            "Epoch: 26, Batch: 800, Loss: 0.0704\n",
            "Epoch: 26, Batch: 900, Loss: 0.0659\n",
            "Epoch: 26, Batch: 1000, Loss: 0.0694\n",
            "Epoch: 26, Batch: 1100, Loss: 0.0631\n",
            "Epoch: 26, Batch: 1200, Loss: 0.0682\n",
            "Epoch: 26, Batch: 1300, Loss: 0.0545\n",
            "Epoch: 26, Batch: 1400, Loss: 0.0735\n",
            "Epoch: 26, Batch: 1500, Loss: 0.0590\n",
            "Epoch 26, Validation Loss: 161.4241, Validation Accuracy: 86.37%\n",
            "Epoch: 27, Batch: 100, Loss: 0.0377\n",
            "Epoch: 27, Batch: 200, Loss: 0.0419\n",
            "Epoch: 27, Batch: 300, Loss: 0.0396\n",
            "Epoch: 27, Batch: 400, Loss: 0.0530\n",
            "Epoch: 27, Batch: 500, Loss: 0.0426\n",
            "Epoch: 27, Batch: 600, Loss: 0.0512\n",
            "Epoch: 27, Batch: 700, Loss: 0.0439\n",
            "Epoch: 27, Batch: 800, Loss: 0.0406\n",
            "Epoch: 27, Batch: 900, Loss: 0.0385\n",
            "Epoch: 27, Batch: 1000, Loss: 0.0389\n",
            "Epoch: 27, Batch: 1100, Loss: 0.0460\n",
            "Epoch: 27, Batch: 1200, Loss: 0.0415\n",
            "Epoch: 27, Batch: 1300, Loss: 0.0405\n",
            "Epoch: 27, Batch: 1400, Loss: 0.0354\n",
            "Epoch: 27, Batch: 1500, Loss: 0.0373\n",
            "Epoch 27, Validation Loss: 174.6774, Validation Accuracy: 86.24%\n",
            "Epoch: 28, Batch: 100, Loss: 0.0296\n",
            "Epoch: 28, Batch: 200, Loss: 0.0242\n",
            "Epoch: 28, Batch: 300, Loss: 0.0262\n",
            "Epoch: 28, Batch: 400, Loss: 0.0312\n",
            "Epoch: 28, Batch: 500, Loss: 0.0237\n",
            "Epoch: 28, Batch: 600, Loss: 0.0250\n",
            "Epoch: 28, Batch: 700, Loss: 0.0297\n",
            "Epoch: 28, Batch: 800, Loss: 0.0245\n",
            "Epoch: 28, Batch: 900, Loss: 0.0242\n",
            "Epoch: 28, Batch: 1000, Loss: 0.0314\n",
            "Epoch: 28, Batch: 1100, Loss: 0.0295\n",
            "Epoch: 28, Batch: 1200, Loss: 0.0299\n",
            "Epoch: 28, Batch: 1300, Loss: 0.0301\n",
            "Epoch: 28, Batch: 1400, Loss: 0.0274\n",
            "Epoch: 28, Batch: 1500, Loss: 0.0234\n",
            "Epoch 28, Validation Loss: 182.8257, Validation Accuracy: 86.45%\n",
            "Epoch: 29, Batch: 100, Loss: 0.0217\n",
            "Epoch: 29, Batch: 200, Loss: 0.0195\n",
            "Epoch: 29, Batch: 300, Loss: 0.0197\n",
            "Epoch: 29, Batch: 400, Loss: 0.0227\n",
            "Epoch: 29, Batch: 500, Loss: 0.0203\n",
            "Epoch: 29, Batch: 600, Loss: 0.0203\n",
            "Epoch: 29, Batch: 700, Loss: 0.0206\n",
            "Epoch: 29, Batch: 800, Loss: 0.0164\n",
            "Epoch: 29, Batch: 900, Loss: 0.0186\n",
            "Epoch: 29, Batch: 1000, Loss: 0.0138\n",
            "Epoch: 29, Batch: 1100, Loss: 0.0178\n",
            "Epoch: 29, Batch: 1200, Loss: 0.0221\n",
            "Epoch: 29, Batch: 1300, Loss: 0.0177\n",
            "Epoch: 29, Batch: 1400, Loss: 0.0195\n",
            "Epoch: 29, Batch: 1500, Loss: 0.0167\n",
            "Epoch 29, Validation Loss: 196.5367, Validation Accuracy: 86.53%\n",
            "Epoch: 30, Batch: 100, Loss: 0.0150\n",
            "Epoch: 30, Batch: 200, Loss: 0.0188\n",
            "Epoch: 30, Batch: 300, Loss: 0.0178\n",
            "Epoch: 30, Batch: 400, Loss: 0.0149\n",
            "Epoch: 30, Batch: 500, Loss: 0.0130\n",
            "Epoch: 30, Batch: 600, Loss: 0.0131\n",
            "Epoch: 30, Batch: 700, Loss: 0.0182\n",
            "Epoch: 30, Batch: 800, Loss: 0.0205\n",
            "Epoch: 30, Batch: 900, Loss: 0.0162\n",
            "Epoch: 30, Batch: 1000, Loss: 0.0153\n",
            "Epoch: 30, Batch: 1100, Loss: 0.0185\n",
            "Epoch: 30, Batch: 1200, Loss: 0.0199\n",
            "Epoch: 30, Batch: 1300, Loss: 0.0160\n",
            "Epoch: 30, Batch: 1400, Loss: 0.0194\n",
            "Epoch: 30, Batch: 1500, Loss: 0.0196\n",
            "Epoch 30, Validation Loss: 208.6678, Validation Accuracy: 86.40%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-319cd1bf82e8>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "net = net.to(device)\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PbJCod0e9kx",
        "outputId": "05540c22-00ac-407b-b6f9-e3dae386902c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 86.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original Implementation has 976K parameters. Since we are using CIFAR10 instead of imageNet, our fully connected layer only has 512 * 10 + 10 parameters instead of 512 * 1000 + 1000. 667914 - 5130 + 513000 = 1175784 parameters. This is around 200,000 more than the original implementation."
      ],
      "metadata": {
        "id": "yyey1Od5V8fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    # Calculate total parameter count\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # Calculate total parameter size in bytes\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "    # Calculate total buffer size in bytes\n",
        "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
        "\n",
        "    # Convert to megabytes\n",
        "    size_all_mb = (param_size + buffer_size) / (1024 ** 2)\n",
        "\n",
        "    print(f\"Model size: {size_all_mb:.2f} MB\")\n",
        "    print(f\"Total number of parameters: {total_params}\")\n",
        "    print()\n",
        "    for name, param in net.named_parameters():\n",
        "        print(f\"{name}: {param.numel()} parameters\")\n",
        "# Call the function for your model\n",
        "get_model_size(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8F8GEt-YqsX",
        "outputId": "673a3af0-bc6a-4ee5-9b4f-2acd5915f646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 2.60 MB\n",
            "Total number of parameters: 667914\n",
            "\n",
            "initial_conv.weight: 864 parameters\n",
            "initial_conv.bias: 32 parameters\n",
            "lb1.0.depthwise1.weight: 288 parameters\n",
            "lb1.0.depthwise1.bias: 32 parameters\n",
            "lb1.0.bn1.weight: 32 parameters\n",
            "lb1.0.bn1.bias: 32 parameters\n",
            "lb1.0.pointwise.weight: 1024 parameters\n",
            "lb1.0.pointwise.bias: 32 parameters\n",
            "lb1.0.bn2.weight: 32 parameters\n",
            "lb1.0.bn2.bias: 32 parameters\n",
            "lb1.0.depthwise2.weight: 288 parameters\n",
            "lb1.0.depthwise2.bias: 32 parameters\n",
            "lb1.0.bn3.weight: 32 parameters\n",
            "lb1.0.bn3.bias: 32 parameters\n",
            "lb1.1.depthwise1.weight: 288 parameters\n",
            "lb1.1.depthwise1.bias: 32 parameters\n",
            "lb1.1.bn1.weight: 32 parameters\n",
            "lb1.1.bn1.bias: 32 parameters\n",
            "lb1.1.pointwise.weight: 1024 parameters\n",
            "lb1.1.pointwise.bias: 32 parameters\n",
            "lb1.1.bn2.weight: 32 parameters\n",
            "lb1.1.bn2.bias: 32 parameters\n",
            "lb1.1.depthwise2.weight: 288 parameters\n",
            "lb1.1.depthwise2.bias: 32 parameters\n",
            "lb1.1.bn3.weight: 32 parameters\n",
            "lb1.1.bn3.bias: 32 parameters\n",
            "lb1.2.depthwise1.weight: 288 parameters\n",
            "lb1.2.depthwise1.bias: 32 parameters\n",
            "lb1.2.bn1.weight: 32 parameters\n",
            "lb1.2.bn1.bias: 32 parameters\n",
            "lb1.2.pointwise.weight: 1024 parameters\n",
            "lb1.2.pointwise.bias: 32 parameters\n",
            "lb1.2.bn2.weight: 32 parameters\n",
            "lb1.2.bn2.bias: 32 parameters\n",
            "lb1.2.depthwise2.weight: 288 parameters\n",
            "lb1.2.depthwise2.bias: 32 parameters\n",
            "lb1.2.bn3.weight: 32 parameters\n",
            "lb1.2.bn3.bias: 32 parameters\n",
            "lb1.3.depthwise1.weight: 288 parameters\n",
            "lb1.3.depthwise1.bias: 32 parameters\n",
            "lb1.3.bn1.weight: 32 parameters\n",
            "lb1.3.bn1.bias: 32 parameters\n",
            "lb1.3.pointwise.weight: 1024 parameters\n",
            "lb1.3.pointwise.bias: 32 parameters\n",
            "lb1.3.bn2.weight: 32 parameters\n",
            "lb1.3.bn2.bias: 32 parameters\n",
            "lb1.3.depthwise2.weight: 288 parameters\n",
            "lb1.3.depthwise2.bias: 32 parameters\n",
            "lb1.3.bn3.weight: 32 parameters\n",
            "lb1.3.bn3.bias: 32 parameters\n",
            "lb2.0.depthwise1.weight: 288 parameters\n",
            "lb2.0.depthwise1.bias: 32 parameters\n",
            "lb2.0.bn1.weight: 32 parameters\n",
            "lb2.0.bn1.bias: 32 parameters\n",
            "lb2.0.pointwise.weight: 4096 parameters\n",
            "lb2.0.pointwise.bias: 128 parameters\n",
            "lb2.0.bn2.weight: 128 parameters\n",
            "lb2.0.bn2.bias: 128 parameters\n",
            "lb2.0.depthwise2.weight: 1152 parameters\n",
            "lb2.0.depthwise2.bias: 128 parameters\n",
            "lb2.0.bn3.weight: 128 parameters\n",
            "lb2.0.bn3.bias: 128 parameters\n",
            "lb2.1.depthwise1.weight: 1152 parameters\n",
            "lb2.1.depthwise1.bias: 128 parameters\n",
            "lb2.1.bn1.weight: 128 parameters\n",
            "lb2.1.bn1.bias: 128 parameters\n",
            "lb2.1.pointwise.weight: 16384 parameters\n",
            "lb2.1.pointwise.bias: 128 parameters\n",
            "lb2.1.bn2.weight: 128 parameters\n",
            "lb2.1.bn2.bias: 128 parameters\n",
            "lb2.1.depthwise2.weight: 1152 parameters\n",
            "lb2.1.depthwise2.bias: 128 parameters\n",
            "lb2.1.bn3.weight: 128 parameters\n",
            "lb2.1.bn3.bias: 128 parameters\n",
            "lb2.2.depthwise1.weight: 1152 parameters\n",
            "lb2.2.depthwise1.bias: 128 parameters\n",
            "lb2.2.bn1.weight: 128 parameters\n",
            "lb2.2.bn1.bias: 128 parameters\n",
            "lb2.2.pointwise.weight: 16384 parameters\n",
            "lb2.2.pointwise.bias: 128 parameters\n",
            "lb2.2.bn2.weight: 128 parameters\n",
            "lb2.2.bn2.bias: 128 parameters\n",
            "lb2.2.depthwise2.weight: 1152 parameters\n",
            "lb2.2.depthwise2.bias: 128 parameters\n",
            "lb2.2.bn3.weight: 128 parameters\n",
            "lb2.2.bn3.bias: 128 parameters\n",
            "lb2.3.depthwise1.weight: 1152 parameters\n",
            "lb2.3.depthwise1.bias: 128 parameters\n",
            "lb2.3.bn1.weight: 128 parameters\n",
            "lb2.3.bn1.bias: 128 parameters\n",
            "lb2.3.pointwise.weight: 16384 parameters\n",
            "lb2.3.pointwise.bias: 128 parameters\n",
            "lb2.3.bn2.weight: 128 parameters\n",
            "lb2.3.bn2.bias: 128 parameters\n",
            "lb2.3.depthwise2.weight: 1152 parameters\n",
            "lb2.3.depthwise2.bias: 128 parameters\n",
            "lb2.3.bn3.weight: 128 parameters\n",
            "lb2.3.bn3.bias: 128 parameters\n",
            "dlb1.0.match_channels.weight: 24576 parameters\n",
            "dlb1.0.match_channels.bias: 192 parameters\n",
            "dlb1.0.depthwise1.weight: 1152 parameters\n",
            "dlb1.0.depthwise1.bias: 128 parameters\n",
            "dlb1.0.bn1.weight: 128 parameters\n",
            "dlb1.0.bn1.bias: 128 parameters\n",
            "dlb1.0.pointwise.weight: 24576 parameters\n",
            "dlb1.0.pointwise.bias: 192 parameters\n",
            "dlb1.0.bn2.weight: 192 parameters\n",
            "dlb1.0.bn2.bias: 192 parameters\n",
            "dlb1.0.depthwise2.weight: 1728 parameters\n",
            "dlb1.0.depthwise2.bias: 192 parameters\n",
            "dlb1.0.bn3.weight: 192 parameters\n",
            "dlb1.0.bn3.bias: 192 parameters\n",
            "dlb1.1.depthwise1.weight: 1728 parameters\n",
            "dlb1.1.depthwise1.bias: 192 parameters\n",
            "dlb1.1.bn1.weight: 192 parameters\n",
            "dlb1.1.bn1.bias: 192 parameters\n",
            "dlb1.1.pointwise.weight: 36864 parameters\n",
            "dlb1.1.pointwise.bias: 192 parameters\n",
            "dlb1.1.bn2.weight: 192 parameters\n",
            "dlb1.1.bn2.bias: 192 parameters\n",
            "dlb1.1.depthwise2.weight: 1728 parameters\n",
            "dlb1.1.depthwise2.bias: 192 parameters\n",
            "dlb1.1.bn3.weight: 192 parameters\n",
            "dlb1.1.bn3.bias: 192 parameters\n",
            "dlb1.2.depthwise1.weight: 1728 parameters\n",
            "dlb1.2.depthwise1.bias: 192 parameters\n",
            "dlb1.2.bn1.weight: 192 parameters\n",
            "dlb1.2.bn1.bias: 192 parameters\n",
            "dlb1.2.pointwise.weight: 36864 parameters\n",
            "dlb1.2.pointwise.bias: 192 parameters\n",
            "dlb1.2.bn2.weight: 192 parameters\n",
            "dlb1.2.bn2.bias: 192 parameters\n",
            "dlb1.2.depthwise2.weight: 1728 parameters\n",
            "dlb1.2.depthwise2.bias: 192 parameters\n",
            "dlb1.2.bn3.weight: 192 parameters\n",
            "dlb1.2.bn3.bias: 192 parameters\n",
            "dlb2.0.match_channels.weight: 49152 parameters\n",
            "dlb2.0.match_channels.bias: 256 parameters\n",
            "dlb2.0.depthwise1.weight: 1728 parameters\n",
            "dlb2.0.depthwise1.bias: 192 parameters\n",
            "dlb2.0.bn1.weight: 192 parameters\n",
            "dlb2.0.bn1.bias: 192 parameters\n",
            "dlb2.0.pointwise.weight: 49152 parameters\n",
            "dlb2.0.pointwise.bias: 256 parameters\n",
            "dlb2.0.bn2.weight: 256 parameters\n",
            "dlb2.0.bn2.bias: 256 parameters\n",
            "dlb2.0.depthwise2.weight: 2304 parameters\n",
            "dlb2.0.depthwise2.bias: 256 parameters\n",
            "dlb2.0.bn3.weight: 256 parameters\n",
            "dlb2.0.bn3.bias: 256 parameters\n",
            "dlb2.1.depthwise1.weight: 2304 parameters\n",
            "dlb2.1.depthwise1.bias: 256 parameters\n",
            "dlb2.1.bn1.weight: 256 parameters\n",
            "dlb2.1.bn1.bias: 256 parameters\n",
            "dlb2.1.pointwise.weight: 65536 parameters\n",
            "dlb2.1.pointwise.bias: 256 parameters\n",
            "dlb2.1.bn2.weight: 256 parameters\n",
            "dlb2.1.bn2.bias: 256 parameters\n",
            "dlb2.1.depthwise2.weight: 2304 parameters\n",
            "dlb2.1.depthwise2.bias: 256 parameters\n",
            "dlb2.1.bn3.weight: 256 parameters\n",
            "dlb2.1.bn3.bias: 256 parameters\n",
            "dlb2.2.match_channels.weight: 131072 parameters\n",
            "dlb2.2.match_channels.bias: 512 parameters\n",
            "dlb2.2.depthwise1.weight: 2304 parameters\n",
            "dlb2.2.depthwise1.bias: 256 parameters\n",
            "dlb2.2.bn1.weight: 256 parameters\n",
            "dlb2.2.bn1.bias: 256 parameters\n",
            "dlb2.2.pointwise.weight: 131072 parameters\n",
            "dlb2.2.pointwise.bias: 512 parameters\n",
            "dlb2.2.bn2.weight: 512 parameters\n",
            "dlb2.2.bn2.bias: 512 parameters\n",
            "dlb2.2.depthwise2.weight: 4608 parameters\n",
            "dlb2.2.depthwise2.bias: 512 parameters\n",
            "dlb2.2.bn3.weight: 512 parameters\n",
            "dlb2.2.bn3.bias: 512 parameters\n",
            "fc.weight: 5120 parameters\n",
            "fc.bias: 10 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "print(os.getcwd())\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgzD-lHkMnG",
        "outputId": "d9e1d258-7b2e-4969-8c6a-18254678f8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net_cpu = net.cpu()\n",
        "torch.save(net_cpu, '/content/drive/My Drive/ECE570/Project/EtinyNetModel.pth')\n",
        "torch.save(net_cpu.state_dict(), '/content/drive/My Drive/ECE570/Project/EtinyNetDict.pth')"
      ],
      "metadata": {
        "id": "DGlqTjy7e_X9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}